{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPnezjkO7IvEi+xGab3QsT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwyalpha/Langchain-Chatchat-Colab/blob/main/Langchain_Chatchat_ChatGLM2_6b_int4_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjYo8f95bPoC"
      },
      "outputs": [],
      "source": [
        "# 拉取仓库\n",
        "!git clone https://github.com/THUDM/ChatGLM2-6B\n",
        "%cd /content/ChatGLM2-6B\n",
        "# int4版本需要安装其中的cpm_kernels\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git\n",
        "%cd /content/langchain-ChatGLM/\n",
        "!pip install -r requirements.txt\n",
        "!pip install --upgrade protobuf==3.19.6\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "!pip install spacy\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "\n",
        "\n",
        "#Download models\n",
        "!git lfs install\n",
        "%cd /content/langchain-ChatGLM\n",
        "!git clone https://huggingface.co/THUDM/chatglm2-6b-int4\n",
        "!git clone https://huggingface.co/moka-ai/m3e-base\n",
        "\n",
        "#复制配置\n",
        "%cp /content/langchain-ChatGLM/configs/model_config.py.example /content/langchain-ChatGLM/configs/model_config.py\n",
        "%cp /content/langchain-ChatGLM/configs/server_config.py.example /content/langchain-ChatGLM/configs/server_config.py\n",
        "#修改model_config.py文件\n",
        "!sed -i \"s/THUDM\\/chatglm2-6b\\\"/\\/content\\/langchain-ChatGLM\\/chatglm2-6b-int4\\\"/g\" /content/langchain-ChatGLM/configs/model_config.py\n",
        "!sed -i \"s/moka-ai\\/m3e-base/\\/content\\/langchain-ChatGLM\\/m3e-base/g\" /content/langchain-ChatGLM/configs/model_config.py\n",
        "\n",
        "\n",
        "%cd /content/langchain-ChatGLM\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime里类型要改成GPU\n",
        "import torch\n",
        "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "# 确认输出是cuda\n",
        "print(EMBEDDING_DEVICE)"
      ],
      "metadata": {
        "id": "pN2ybDqfTybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python init_database.py --recreate-vs\n",
        "!nohup python server/llm_api.py &"
      ],
      "metadata": {
        "id": "zI5YWxC-SD2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 稍等上一步跑完再执行\n",
        "!nohup python server/api.py &"
      ],
      "metadata": {
        "id": "s2u7XR4Tbe7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run webui.py &>/dev/null&"
      ],
      "metadata": {
        "id": "vmxB4WCAN2Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npm install localtunnel\n",
        "#!streamlit run app.py &>web_logs.txt &\n",
        "!npx localtunnel --port 8501\n",
        "# 在网页中输入IP后提交"
      ],
      "metadata": {
        "id": "wQT4wesHPW09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}