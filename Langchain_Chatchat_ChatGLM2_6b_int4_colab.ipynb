{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPnezjkO7IvEi+xGab3QsT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwyalpha/Langchain-Chatchat-Colab/blob/main/Langchain_Chatchat_ChatGLM2_6b_int4_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjYo8f95bPoC"
      },
      "outputs": [],
      "source": [
        "# 拉取仓库\n",
        "!git clone https://github.com/THUDM/ChatGLM2-6B\n",
        "%cd /content/ChatGLM2-6B\n",
        "# int4版本需要安装其中的cpm_kernels\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git\n",
        "%cd /content/langchain-ChatGLM/\n",
        "!pip install -r requirements.txt\n",
        "!pip install --upgrade protobuf==3.19.6\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "!pip install spacy\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "\n",
        "\n",
        "#Download models\n",
        "!git lfs install\n",
        "%cd /content/langchain-ChatGLM\n",
        "!git clone https://huggingface.co/THUDM/chatglm2-6b-int4\n",
        "!git clone https://huggingface.co/moka-ai/m3e-base\n",
        "\n",
        "#复制配置\n",
        "%cp /content/langchain-ChatGLM/configs/model_config.py.example /content/langchain-ChatGLM/configs/model_config.py\n",
        "%cp /content/langchain-ChatGLM/configs/server_config.py.example /content/langchain-ChatGLM/configs/server_config.py\n",
        "#修改model_config.py文件\n",
        "!sed -i \"s/THUDM\\/chatglm2-6b\\\"/\\/content\\/langchain-ChatGLM\\/chatglm2-6b-int4\\\"/g\" /content/langchain-ChatGLM/configs/model_config.py\n",
        "!sed -i \"s/moka-ai\\/m3e-base/\\/content\\/langchain-ChatGLM\\/m3e-base/g\" /content/langchain-ChatGLM/configs/model_config.py\n",
        "\n",
        "\n",
        "%cd /content/langchain-ChatGLM\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime里类型要改成GPU\n",
        "import torch\n",
        "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "# 确认输出是cuda\n",
        "print(EMBEDDING_DEVICE)"
      ],
      "metadata": {
        "id": "pN2ybDqfTybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python init_database.py --recreate-vs\n",
        "!nohup python server/llm_api.py &"
      ],
      "metadata": {
        "id": "zI5YWxC-SD2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 稍等上一步跑完再执行\n",
        "!nohup python server/api.py &"
      ],
      "metadata": {
        "id": "s2u7XR4Tbe7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run webui.py &>/dev/null&"
      ],
      "metadata": {
        "id": "vmxB4WCAN2Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npm install localtunnel\n",
        "#!streamlit run app.py &>web_logs.txt &\n",
        "!npx localtunnel --port 8501\n",
        "# 在网页中输入IP后提交"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQT4wesHPW09",
        "outputId": "f3699d4d-3274-4e4a-d1a7-de0305f72420"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.91.53.228\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/langchain-ChatGLM/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/langchain-ChatGLM/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m langchain-ChatGLM No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m langchain-ChatGLM No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m langchain-ChatGLM No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m langchain-ChatGLM No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 1.279s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.259s\n",
            "your url is: https://legal-poets-add.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}