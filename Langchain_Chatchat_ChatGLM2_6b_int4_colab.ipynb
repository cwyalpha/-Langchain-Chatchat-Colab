{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/uj1zl3pfmSb2U+v8L95a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwyalpha/Langchain-Chatchat-Colab/blob/main/Langchain_Chatchat_ChatGLM2_6b_int4_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "# 首先确认输出是cuda，否则Runtime里类型要改成T4 GPU\n",
        "print(EMBEDDING_DEVICE)\n",
        "\n",
        "# 拉取仓库\n",
        "!git clone https://github.com/THUDM/ChatGLM2-6B\n",
        "%cd /content/ChatGLM2-6B\n",
        "# int4版本需要安装其中的cpm_kernels\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 基于Langchain-Chatchat 0.2.6测试通过\n",
        "%cd /content/\n",
        "!git clone https://github.com/chatchat-space/Langchain-Chatchat.git\n",
        "%cd /content/Langchain-Chatchat/\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r requirements_api.txt\n",
        "!pip install -r requirements_webui.txt\n",
        "!pip install --upgrade protobuf==3.19.6\n",
        "!pip install gradio\n",
        "!pip install accelerate\n",
        "!pip install spacy\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "\n",
        "\n",
        "# Download models\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/THUDM/chatglm2-6b-int4\n",
        "!git clone https://huggingface.co/moka-ai/m3e-base\n",
        "\n",
        "# 复制配置\n",
        "%cd /content/Langchain-Chatchat/\n",
        "# 复制配置文件\n",
        "!python copy_config_example.py\n",
        "# 修改model_config.py文件\n",
        "!sed -i \"s/THUDM\\/chatglm2-6b-int4\\\"/\\/content\\/Langchain-Chatchat\\/chatglm2-6b-int4\\\"/g\" /content/Langchain-Chatchat/configs/model_config.py\n",
        "!sed -i 's/\\(LLM_MODEL\\s*=\\s*\\)\"chatglm2-6b\"/\\1\"chatglm2-6b-int4\"/' /content/Langchain-Chatchat/configs/model_config.py\n",
        "!sed -i \"s/moka-ai\\/m3e-base/\\/content\\/Langchain-Chatchat\\/m3e-base/g\" /content/Langchain-Chatchat/configs/model_config.py\n",
        "# 初始化知识库\n",
        "!python init_database.py --recreate-vs\n",
        "# 启动项目\n",
        "!nohup python startup.py -a &\n",
        "\n",
        "# 最后一步nohup运行之后，需要等待一段时间网页内才有内容\n",
        "# 在/content/Langchain-Chatchat/nohup.out中看到0.0.0.0:8501后就可以正常访问\n",
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npm install localtunnel\n",
        "#!streamlit run app.py &>web_logs.txt &\n",
        "!npx localtunnel --port 8501\n",
        "# 在网页中输入IP后提交"
      ],
      "metadata": {
        "id": "l2upTF6Aebmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}